{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Копия блокнота \"vae.ipynb\"",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmykhTvUH5VK"
      },
      "source": [
        "# Variational Autoencoder (VAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgP7Ss_OH5VL"
      },
      "source": [
        "Useful links: \n",
        "* Original paper http://arxiv.org/abs/1312.6114\n",
        "* Helpful videos explaining the topic \n",
        "   * https://www.youtube.com/watch?v=P78QYjWh5sM     \n",
        "   * http://videolectures.net/deeplearning2015_courville_autoencoder_extension/?q=aaron%20courville"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQAiMcqWH5VM"
      },
      "source": [
        "In this homework we will train an autoencoder to model images of faces. For this we take \"Labeled Faces in the Wild\" dataset [http://vis-www.cs.umass.edu/lfw/](http://vis-www.cs.umass.edu/lfw/), deep funneled version of it. (frontal view of all faces)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQQtDFR0H5VM"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdA5qLsH5VN"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfp9aP4vH5VN"
      },
      "source": [
        "# The following code fetches you two datasets: images, usable for autoencoder training and attributes.\n",
        "# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
        "from lfw_dataset import fetch_lfw_dataset\n",
        "    \n",
        "data, attrs = fetch_lfw_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8PgicOKxH5VO",
        "outputId": "df8d2c43-d2aa-49c4-bf9b-9e4e0f65e2c9"
      },
      "source": [
        "X_train = data[:10000].reshape((10000, -1))\n",
        "print(X_train.shape)\n",
        "X_val = data[10000:].reshape((-1, X_train.shape[1]))\n",
        "print(X_val.shape)\n",
        "\n",
        "image_h = data.shape[1]\n",
        "image_w = data.shape[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 6075)\n",
            "(3143, 6075)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DY7qkmVH5VO"
      },
      "source": [
        "size = X_train[0].shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YxSMw_7H5VP"
      },
      "source": [
        "For simplicity we want all values of the data to lie in the interval $[0,1]$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_PWhUAGH5VP"
      },
      "source": [
        "X_train = np.float32(X_train)\n",
        "X_train = X_train/255\n",
        "X_val = np.float32(X_val)\n",
        "X_val = X_val/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwIjwrqTH5VP"
      },
      "source": [
        "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
        "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
        "    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ubpQucH5VP",
        "outputId": "94391adf-d220-464c-f7a6-9198c12763c9"
      },
      "source": [
        "plot_gallery(X_train, image_h, image_w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAosAAAFcCAYAAACgOfQdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHsElEQVR4nO3dQYrbSBiAUWnwFXrd9z+W975DZdPpTGA+GDmylLLeA+9EUfAv9FGW5XWMsQAAwH/55+wNAADw9xKLAAAksQgAQBKLAAAksQgAQBKLAACk25aL13X1np0DjDHWvdYys2PsNTPzOsxjjPGxx0Jmdhgzm4x72XxqZk4WgSu6n70BNjMzOIlYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAgiUUAAJJYBAAg3TZe/1iW5f6KjfDtc+f1zOz19pyZeR3DzOZjZnNxL5tPzmwdYxy5EQAAJuJraAAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAAklgEACCJRQAA0m3Lxeu6jldthF/GGOtea5nZMfaamXkd5jHG+NhjITM7jJlNxr1sPjUzJ4vAFd3P3gCbmRmcRCwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCXMr69QH4f8QiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAADpdvYGADjSOHsDwGScLAIAkMQiwNTWr8+s6wN/O7EIAEDyzCLA1F79DKJnHOHqnCwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJBuG69/LMtyf8VG+Pa583pm9np7zsy8jmFm8zGzubiXzSdnto4xjtwIAAAT8TU0AABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAABJLAIAkMQiAADptuXidV3HqzbCL2OMda+1zOwYe83MvA7zGGN87LGQmR3GzCbjXjafmpmTReCK7mdvgM3MDE4iFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRAIAkFgEASGIRnrJ+fQDgvYlFAADS7ewNwJzG2RsAgEM4WQQAIIlFAACSWAQAIIlFAACSWAQAIIlFAACSWAQAIHnPIjzl3//e4p2LALwvJ4sAACSxCH/EqSIA700sAgCQPLMIT3GiCMA1OFkEACCJRQAAklgEACCJRXjKuvz+rkUAeE9iEQCAJBbhKePr43QRgPcmFgEASGIRnvH9yKL3LQLw3sQiAADJP7jAMxwoAnARThYBAEhiEf6IX0MD8N7EIgAAyTOLsImTRACuxckiAABJLMImP/+5BQCuQSwCAJA8swhPcboIwDU4WQQAIIlFAACSWAQAIIlFAACSWAQAIIlFAACSWAQAIIlFAACSWAQAIIlFAACSWAQAIG39b+jHsiz3V2yEb587r2dmr7fnzMzrGGY2HzObi3vZfHJm6xjjyI0AADARX0MDAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJDEIgAASSwCAJB+AOxykqXBSwRPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x367.2 with 18 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eQ97as9H5VQ"
      },
      "source": [
        "# Autoencoder\n",
        "\n",
        "Why to use all this complicated formulaes and regularizations, what is the need for variational inference? To analyze the difference, let's first train just an autoencoder on the data:\n",
        "\n",
        "<img src=\"Autoencoder_structure.png\" alt=\"Autoencoder\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9rI-ZRoH5VQ",
        "outputId": "cae0a796-8639-4d4f-8744-1790995e4b62"
      },
      "source": [
        "import tensorflow\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Arman Tigranyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Не найден указанный модуль.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Не найден указанный модуль.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-26-57054b509480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# go/tf-wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Arman Tigranyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Не найден указанный модуль.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OPRuZUwH5VQ",
        "outputId": "a08cdc40-6be0-4fe4-bd15-0e479099a2b4"
      },
      "source": [
        "X_train = tf.convert_to_tensor(X_train)\n",
        "X_test = tf.convert_to_tensor(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-5c96dfdf6356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdaDhIWVH5VR"
      },
      "source": [
        "dimZ = 100 # Considering face reconstruction task, which size of representation seems reasonable?\n",
        "model =Sequential()\n",
        "model.add(Dense(700, input_shape=size, activation='relu'))\n",
        "model.add(Dense(dimZ, activation='sigmoid'))\n",
        "# Define the decoder and encoder as networks with one hidden fc-layer\n",
        "# (that means you will have 2 fc layers in each net)\n",
        "# Use ReLU for hidden layers' activations\n",
        "# GlorotUniform initialization for W\n",
        "# Zero initialization for biases\n",
        "# It's also convenient to put sigmoid activation on output layer to get nice normalized pics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dH-nP0uH5VR"
      },
      "source": [
        "# Create MSE loss function\n",
        "\n",
        "# Use Adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dhtntjeVH5VR"
      },
      "source": [
        "# Train your autoencoder\n",
        "# Visualize progress in reconstruction and loss decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_8b4F7-CH5VS"
      },
      "source": [
        "# Examine the reconstructions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V21OGiKH5VS"
      },
      "source": [
        "Reconstruction is not bad, right? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBIw2SmKH5VS"
      },
      "source": [
        "# Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yjDxugH5VS"
      },
      "source": [
        "Let's now sample several latent vectors and perform inference from $z$, reconstruct an image given some random $z$ representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5E-MnF1H5VT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mqmbuImVH5VT"
      },
      "source": [
        "z = np.random.randn(25, dimZ)*0.5\n",
        "output = decoder(z)\n",
        "plot_gallery(output, image_h, image_w, n_row=5, n_col=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z9XntXJH5VT"
      },
      "source": [
        "So, if we sample $z$ from normal, whould we eventually generate all possible faces? What do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ap32xxpH5VT"
      },
      "source": [
        "# Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsy2wrjzH5VT"
      },
      "source": [
        "Bayesian approach in deep learning considers everything in terms of distributions. Now our encoder generates not just a vector $z$ but posterior ditribution $q(z|x)$. In our case distribution $q$ is Gaussian distibution $N(\\mu, \\sigma)$ with parameters $\\mu$, $\\sigma$. Technically, the first difference is that you need to split bottleneck layer in two. One dense layer will generate vector $\\mu$, and another will generate vector $\\sigma$. Reparametrization trick should be implemented using the **gaussian_sampler**, that generates random vetor $\\epsilon$ and returns $\\mu+\\sigma\\epsilon \\sim N(\\mu, \\sigma)$ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Yi0EehH5VU"
      },
      "source": [
        "Since our decoder is also a function that generates distribution, we need to do the same splitting for output layer. When testing the model we will look only on mean values, so one of the output will be actual autoencoder output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySCuD2ZCH5VU"
      },
      "source": [
        "In this homework we only ask for implementation of the simplest version of VAE - one $z$ sample per input. You can consider to sample several outputs from one input and average them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh34jM2_H5VU"
      },
      "source": [
        "# to compare with conventional AE, keep these hyperparameters\n",
        "# or change them for the values that you used before\n",
        "dimZ = 100\n",
        "\n",
        "# define the network\n",
        "# you can start from pytorch example https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "# or Theano-based examples here https://github.com/Lasagne/Recipes/blob/master/examples/variational_autoencoder/variational_autoencoder.py\n",
        "# and here https://github.com/y0ast/Variational-Autoencoder/blob/master/VAE.py\n",
        "# but remember that this is not your ground truth since the data is not MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18X2uk51H5VU",
        "outputId": "cff7d063-97b6-4725-c718-8608eb99f728"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.layers.Dense(1024,activation= 'relu')\n",
        "        self.fc2 = tf.layers.Dense(2,activation= None)\n",
        "        self.fc3 = tf.layers.Dense(2,activation= None)\n",
        "    \n",
        "    def call(self,inp):\n",
        "        h = self.fc1(inp)\n",
        "        z_mean = self.fc2(h)\n",
        "        z_log_sigma = self.fc3(h)\n",
        "        batch_size = inp.shape[0]\n",
        "        rnd = tf.random.normal(tf.shape(z_mean))\n",
        "        z = z_mean + tf.exp(z_log_sigma)* rnd\n",
        "        return  z_mean,z_log_sigma,z\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.layers.Dense(1024,activation= 'relu')\n",
        "        self.fc2 = tf.layers.Dense(size[0],activation= 'sigmoid')\n",
        "    \n",
        "    \n",
        "    def call(self,inp):\n",
        "        h = self.fc1(inp)\n",
        "        out = self.fc2(h)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-24-e0b99aaa5d9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zilAaj-HH5VV",
        "outputId": "6bbf5cf7-daed-48d0-fe73-0596a55c2440"
      },
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Encoder' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-25-c94e0ba4bed9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEhCbRuFH5VV"
      },
      "source": [
        "And the last, but not least! Place in the code where the most of the formulaes goes to - optimization objective. The objective for VAE has it's own name - variational lowerbound. And as for any lowerbound our intention is to maximize it. Here it is (for one sample $z$ per input $x$):\n",
        "\n",
        "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z)) + \\log p_{\\theta}(x|z)$$\n",
        "\n",
        "Your next task is to implement two functions that compute KL-divergence and the second term - log-likelihood of an output. Here is some necessary math for your convenience:\n",
        "\n",
        "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
        "$$\\log p_{\\theta}(x|z) = \\sum_{i=1}^{dimX}\\log p_{\\theta}(x_i|z)=\\sum_{i=1}^{dimX} \\log \\Big( \\frac{1}{\\sigma_i\\sqrt{2\\pi}}e^{-\\frac{(\\mu_I-x)^2}{2\\sigma_i^2}} \\Big)=...$$\n",
        "\n",
        "Don't forget in the code that you are using $\\log\\sigma$ as variable. Explain, why not $\\sigma$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaq3fffTH5VV"
      },
      "source": [
        "def KL_divergence(mu, logsigma):\n",
        "    tmp = 1 + logsigma - tf.square(mu) - tf.exp(logsigma)\n",
        "    return -0.5 * tf.reduse_sum(tmp,axis = -1)\n",
        "\n",
        "def log_likelihood(x, mu, logsigma):\n",
        "    return tf.reduse_sum(tf.math.log(1/(2*math.pi ** 0.5 * tf.exp(logsigma)))\\\n",
        "                         *tf.exp(tf.square(mu-x,axis = 1)/2/tf.exp(tf.square(logsigma))))\n",
        "\n",
        "def loss_vae(x, mu_gen, logsigma_gen, mu_z, logsigma_z):\n",
        "    return tf.reduce_mean(tf.square(mu_z-x,axis = 1) + KL_divergence(mu_z, logsigma_z)\\\n",
        "                          +tf.square(mu_gen-x,axis = 1) + KL_divergence(mu_gen, logsigma_gen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRrOK-rRH5VW"
      },
      "source": [
        "And train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m12WfIpH5VW",
        "outputId": "d156de1e-a625-4538-aa68-56055db366a0"
      },
      "source": [
        "# train your autoencoder\n",
        "epochs = 25 \n",
        "butch_size = 128\n",
        "lr = 0.001\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "train_ds = train_ds.shuffle(buffer_size = train_x.shape[0])\n",
        "train_ds = train_ds.batch(butch_size,drop_remainder = True)\n",
        "# visualize progress in reconstruction and loss decay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-2c543f6fd82a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbutch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbutch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_remainder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vXxzVzDH5VX"
      },
      "source": [
        "for e in range(epochs):\n",
        "    for batch in train_ds:\n",
        "        with tf.GradientTape() as tape():\n",
        "            encoder.call(batch)\n",
        "            loss_value = loss_vae()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUwb0i9cH5VX"
      },
      "source": [
        "# test your autoencoder with validation data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REoVL-bdH5VY"
      },
      "source": [
        "And finally sample from VAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X8lRb3zH5VY"
      },
      "source": [
        "# TODO\n",
        "# Sample some images from the learned distribution\n",
        "# 1) Sample z ~ N(0,1)\n",
        "# 2) Sample from N(decoder_mu(z), decoder_sigma(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovtVStzfH5VZ"
      },
      "source": [
        "Even if in practice you do not see the much difference between AE and VAE, or VAE is even worse, the little bayesian inside you should be jumping for joy right now. \n",
        "\n",
        "In VAE you can truly sample from image distribution $p(x)$, while in AE there is no easy and correct way to do it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKKy3HuFH5VZ"
      },
      "source": [
        "# Congrats! and Bonus\n",
        "\n",
        "If you managed to tune your autoencoders to converge and learn something about the world, now it's time to make fun out of it. As you may have noticed, there are face attributes in dataset. We're interesting in \"Smiling\" column, but feel free to try others as well! Here is the first task:\n",
        "\n",
        "1) Extract the \"Smilling\" attribute and create a two sets of images: 10 smiling faces and 10 non-smiling ones.\n",
        "\n",
        "2) Compute latent representations for each image in \"smiling\" set and average those latent vectors. Do the same for \"non-smiling\" set. You have found **\"vector representation\"** of the \"smile\" and \"no smile\" attribute.\n",
        "\n",
        "3) Compute the difference: \"smile\" vector minus \"non-smile\" vector.\n",
        "\n",
        "3) Now check if **\"feature arithmetics\"** works. Sample a face without smile, encode it and add the diff from p. 3. Check if it works with both AE and VAE. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzO9I7r2H5VZ"
      },
      "source": [
        "<img src=\"linear.png\" alt=\"linear\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwE6Smo_H5Va"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}